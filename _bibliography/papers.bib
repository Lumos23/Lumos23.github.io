---
---

selected = true will show up on the first page

# 2025
@article{gao2025metadataconditioningaccelerateslanguage,
      title={Metadata Conditioning Accelerates Language Model Pre-training}, 
      author={Tianyu Gao and Alexander Wettig and Luxi He and Yihe Dong and Sadhika Malladi and Danqi Chen},
      year=2025,
      journal={Preprint},
      selected={False},
      arxiv={2501.01956},
      preview={MeCo_cover.png}, 
      code={https://github.com/princeton-pli/MeCo},
      abstract={The vast diversity of styles, domains, and quality levels present in language model pre-training corpora is essential in developing general model capabilities, but efficiently learning and deploying the correct behaviors exemplified in each of these heterogeneous data sources is challenging. To address this, we propose a new method, termed Metadata Conditioning then Cooldown (MeCo), to incorporate additional learning cues during pretraining. MeCo first provides metadata (e.g., URLs like en.wikipedia.org) alongside the text during training and later uses a cooldown phase with only the standard text, thereby enabling the model to function normally even without metadata. MeCo significantly accelerates pre-training across different model scales (600M to 8B parameters) and training sources (C4, RefinedWeb, and DCLM). For instance, a 1.6B language model trained with MeCo matches the downstream task performance of standard pretraining while using 33% less data. Additionally, MeCo enables us to steer language models by conditioning the inference prompt on either real or fabricated metadata that encodes the desired properties of the output: for example, prepending wikipedia.org to reduce harmful generations or factquizmaster.com (fabricated) to improve common knowledge task performance. We also demonstrate that MeCo is compatible with different types of metadata, such as model-generated topics. MeCo is remarkably simple, adds no computational overhead, and demonstrates promise in producing more capable and steerable language models.}
}

# 2024
@article{he2024safedataidentifyingbenign,
  title={What is in Your Safe Data? Identifying Benign Data that Breaks Safety}, 
  author={Luxi He* and Mengzhou Xia* and Peter Henderson},
  year=2024,
  journal      = {Conference on Language Modeling (COLM), ICLR Data Problems in Foundation Model <span style="font-weight:bold; color:red;">(Best Paper)</span>},
  selected     = {true},
	preview		 = {benign_data_safety.png},
  arxiv = {2404.01099},
  code = {https://github.com/princeton-nlp/benign-data-breaks-safety},
  slides = {ICLR_data_selection_safety_final.pdf},
  abstract = {Current Large Language Models (LLMs), even those tuned for safety and alignment, are susceptible to jailbreaking. Some have found that just further fine-tuning an aligned model with benign data (i.e., data without harmful content) surprisingly leads to substantial degradation in safety. We delve into the data-centric aspects of why benign fine-tuning inadvertently contributes to jailbreaking. First, we represent fine-tuning data through two lenses: representation and gradient spaces. Furthermore, we propose a bi-directional anchoring method that prioritizes data points that are close to harmful examples and distant from benign ones. By doing so, our approach effectively identifies subsets of benign data that are more likely to degrade the model's safety after fine-tuning. Training on just 100 of these seemingly benign datapoints can lead to the fine-tuned model affirmatively responding to > 70% of tested harmful requests, compared to < 20% after fine-tuning on randomly selected data. We further find that selected data are often in the form of lists and bullet points, or math questions.}
}

@article{he2024fantasticcopyrightedbeastsnot,
  title={Fantastic Copyrighted Beasts and How (Not) to Generate Them}, 
  author={Luxi He* and Yangsibo Huang* and Weijia Shi* and Tinghao Xie and Haotian Liu and Yue Wang and Luke Zettlemoyer and Chiyuan Zhang and Danqi Chen and Peter Henderson},
  year=2024,
  journal      = {ICML GenLaw <span style="font-weight:bold; color:red;">(Spotlight)</span>},
  selected     = {true},
	preview		 = {copycat_cover.png},
  arxiv = {2406.14526},
  code = {https://github.com/princeton-nlp/CopyCat},
  website = {https://copycat-eval.github.io/},
  abstract = {Recent studies show that image and video generation models can be prompted to reproduce copyrighted content from their training data, raising serious legal concerns around copyright infringement. Copyrighted characters, in particular, pose a difficult challenge for image generation services, with at least one lawsuit already awarding damages based on the generation of these characters. Yet, little research has empirically examined this issue. We conduct a systematic evaluation to fill this gap. First, we build CopyCat, an evaluation suite consisting of diverse copyrighted characters and a novel evaluation pipeline. Our evaluation considers both the detection of similarity to copyrighted characters and generated image's consistency with user input. Our evaluation systematically shows that both image and video generation models can still generate characters even if characters' names are not explicitly mentioned in the prompt, sometimes with only two generic keywords (e.g., prompting with "videogame, plumber" consistently generates Nintendo's Mario character). We then introduce techniques to semi-automatically identify such keywords or descriptions that trigger character generation. Using our evaluation suite, we study runtime mitigation strategies, including both existing methods and new strategies we propose. Our findings reveal that commonly employed strategies, such as prompt rewriting in the DALL-E system, are not sufficient as standalone guardrails. These strategies must be coupled with other approaches, like negative prompting, to effectively reduce the unintended generation of copyrighted characters. Our work provides empirical grounding to the discussion of copyright mitigation strategies and offers actionable insights for model deployers actively implementing them.}
}



@article{wang2024charxivchartinggapsrealistic,
  title={CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs}, 
  author={Zirui Wang and Mengzhou Xia and Luxi He and Howard Chen and Yitao Liu and Richard Zhu and Kaiqu Liang and Xindi Wu and Haotian Liu and Sadhika Malladi and Alexis Chevalier and Sanjeev Arora and Danqi Chen},
  year=2024,
  journal      = {NeurIPS Datasets & Benchmarks},
  selected     = {true},
	preview		 = {benign_data_safety.png},
  arxiv = {2406.18521},
  website = {https://charxiv.github.io/},
  dataset = {https://huggingface.co/datasets/princeton-nlp/CharXiv},
  preview = {charxiv_cover.png},
  abstract = {Chart understanding plays a pivotal role when applying Multimodal Large Language Models (MLLMs) to real-world tasks such as analyzing scientific papers or financial reports. However, existing datasets often focus on oversimplified and homogeneous charts with template-based questions, leading to an over-optimistic measure of progress. We demonstrate that although open-source models can appear to outperform strong proprietary models on these benchmarks, a simple stress test with slightly different charts or questions can deteriorate performance by up to 34.5%. In this work, we propose CharXiv, a comprehensive evaluation suite involving 2,323 natural, challenging, and diverse charts from arXiv papers. CharXiv includes two types of questions: 1) descriptive questions about examining basic chart elements and 2) reasoning questions that require synthesizing information across complex visual elements in the chart. To ensure quality, all charts and questions are handpicked, curated, and verified by human experts. Our results reveal a substantial, previously underestimated gap between the reasoning skills of the strongest proprietary model (i.e., GPT-4o), which achieves 47.1% accuracy, and the strongest open-source model (i.e., InternVL Chat V1.5), which achieves 29.2%. All models lag far behind human performance of 80.5%, underscoring weaknesses in the chart understanding capabilities of existing MLLMs. We hope CharXiv facilitates future research on MLLM chart understanding by providing a more realistic and faithful measure of progress.}
}


# 2023
@inproceedings{NEURIPS2023_55a49718,
 author = {Wang, Hao and He, Luxi and Gao, Rui and Calmon, Flavio},
 booktitle = {NeurIPS <span style="font-weight:bold; color:red;">(Spotlight)</span>},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {27040--27062},
 publisher = {Curran Associates, Inc.},
 title = {Aleatoric and Epistemic Discrimination: Fundamental Limits of Fairness Interventions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/55a49718689fdecef31b6a2386df6fe1-Paper-Conference.pdf},
 volume = {36},
 year = {2023},
 preview = {fairfront_cover.png},
 arxiv = {2301.11781},
 code = {https://github.com/Lumos23/aleatoric_epistemic_discrimination},
 selected = {true},
 abstract = {Machine learning (ML) models can underperform on certain population groups due to choices made during model development and bias inherent in the data. We categorize sources of discrimination in the ML pipeline into two classes: aleatoric discrimination, which is inherent in the data distribution, and epistemic discrimination, which is due to decisions made during model development. We quantify aleatoric discrimination by determining the performance limits of a model under fairness constraints, assuming perfect knowledge of the data distribution. We demonstrate how to characterize aleatoric discrimination by applying Blackwell's results on comparing statistical experiments. We then quantify epistemic discrimination as the gap between a model's accuracy when fairness constraints are applied and the limit posed by aleatoric discrimination. We apply this approach to benchmark existing fairness interventions and investigate fairness risks in data with missing values. Our results indicate that state-of-the-art fairness interventions are effective at removing epistemic discrimination on standard (overused) tabular datasets. However, when data has missing values, there is still significant room for improvement in handling aleatoric discrimination.}
}




